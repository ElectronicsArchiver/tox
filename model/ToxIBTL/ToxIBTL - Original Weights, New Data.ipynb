{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: this notebook's purpose is to evaluate the original ToxIBTL model on our new test data\n",
    "## hence, the code for training ToxIBTL is NOT included in this notebook\n",
    "## moreover, the original code has been modified to facilitate the notebook's purpose\n",
    "## to view the originl code from ToxIBTL, please see ToxIBTL - Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "import torch.optim as opt\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "from torch.nn.utils.rnn import pad_packed_sequence,pack_padded_sequence\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.io as sio\n",
    "import pickle\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "blosum62 = {\n",
    "        'A': [4, -1, -2, -2, 0,  -1, -1, 0, -2,  -1, -1, -1, -1, -2, -1, 1,  0,  -3, -2, 0],  # A\n",
    "        'R': [-1, 5,  0, -2, -3, 1,  0,  -2, 0,  -3, -2, 2,  -1, -3, -2, -1, -1, -3, -2, -3], # R\n",
    "        'N': [-2, 0,  6,  1,  -3, 0,  0,  0,  1,  -3, -3, 0,  -2, -3, -2, 1,  0,  -4, -2, -3], # N\n",
    "        'D': [-2, -2, 1,  6,  -3, 0,  2,  -1, -1, -3, -4, -1, -3, -3, -1, 0,  -1, -4, -3, -3], # D\n",
    "        'C': [0,  -3, -3, -3, 9,  -3, -4, -3, -3, -1, -1, -3, -1, -2, -3, -1, -1, -2, -2, -1], # C\n",
    "        'Q': [-1, 1,  0,  0,  -3, 5,  2,  -2, 0,  -3, -2, 1,  0,  -3, -1, 0,  -1, -2, -1, -2], # Q\n",
    "        'E': [-1, 0,  0,  2,  -4, 2,  5,  -2, 0,  -3, -3, 1,  -2, -3, -1, 0,  -1, -3, -2, -2], # E\n",
    "        'G': [0,  -2, 0,  -1, -3, -2, -2, 6,  -2, -4, -4, -2, -3, -3, -2, 0,  -2, -2, -3, -3], # G\n",
    "        'H': [-2, 0,  1,  -1, -3, 0,  0,  -2, 8,  -3, -3, -1, -2, -1, -2, -1, -2, -2, 2,  -3], # H\n",
    "        'I': [-1, -3, -3, -3, -1, -3, -3, -4, -3, 4,  2,  -3, 1,  0,  -3, -2, -1, -3, -1, 3],  # I\n",
    "        'L': [-1, -2, -3, -4, -1, -2, -3, -4, -3, 2,  4,  -2, 2,  0,  -3, -2, -1, -2, -1, 1],  # L\n",
    "        'K': [-1, 2,  0,  -1, -3, 1,  1,  -2, -1, -3, -2, 5,  -1, -3, -1, 0,  -1, -3, -2, -2], # K\n",
    "        'M': [-1, -1, -2, -3, -1, 0,  -2, -3, -2, 1,  2,  -1, 5,  0,  -2, -1, -1, -1, -1, 1],  # M\n",
    "        'F': [-2, -3, -3, -3, -2, -3, -3, -3, -1, 0,  0,  -3, 0,  6,  -4, -2, -2, 1,  3,  -1], # F\n",
    "        'P': [-1, -2, -2, -1, -3, -1, -1, -2, -2, -3, -3, -1, -2, -4, 7,  -1, -1, -4, -3, -2], # P\n",
    "        'S': [1,  -1, 1,  0,  -1, 0,  0,  0,  -1, -2, -2, 0,  -1, -2, -1, 4,  1,  -3, -2, -2], # S\n",
    "        'T': [0,  -1, 0,  -1, -1, -1, -1, -2, -2, -1, -1, -1, -1, -2, -1, 1,  5,  -2, -2, 0],  # T\n",
    "        'W': [-3, -3, -4, -4, -2, -2, -3, -2, -2, -3, -2, -3, -1, 1,  -4, -3, -2, 11, 2,  -3], # W\n",
    "        'Y': [-2, -2, -2, -3, -2, -1, -2, -3, 2,  -1, -1, -2, -1, 3,  -3, -2, -2, 2,  7,  -1], # Y\n",
    "        'V': [0,  -3, -3, -3, -1, -2, -2, -3, -3, 3,  1,  -2, 1,  -1, -2, -2, 0,  -3, -1, 4],  # V\n",
    "        '-': [0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],  # -\n",
    "    }\n",
    "\n",
    "def normalization(dataset):\n",
    "    min = dataset.min(axis=0)\n",
    "    max = dataset.max(axis=0)\n",
    "    dataset = (dataset - min) / (max - min)\n",
    "    return dataset\n",
    "\n",
    "def get_blosum62(seq):\n",
    "    blosum_list = []\n",
    "    for i in seq:\n",
    "        if i in blosum62:\n",
    "            blosum_list.append(blosum62[i])\n",
    "        else:\n",
    "            blosum_list.append(blosum62['-'])\n",
    "    blosum = np.array(blosum_list)\n",
    "#     blosum = normalization(blosum)  # why did they get ride of normalization?\n",
    "    feature = np.zeros((1002,20))\n",
    "    idx = blosum.shape[0]\n",
    "    feature[0:idx,:] = blosum\n",
    "    return feature\n",
    "\n",
    "\n",
    "def make_tensor(path):\n",
    "    data = pd.read_csv(path)\n",
    "    sequences = data['sequence'].values\n",
    "    labels = data['label'].values\n",
    "    evolution = torch.zeros(len(sequences),1002,20)\n",
    "    lengths = []\n",
    "    for i in range(len(sequences)):\n",
    "        lengths.append((len(sequences[i])))\n",
    "        temp = get_blosum62(sequences[i])\n",
    "        evolution[i,:,:] = torch.Tensor(temp)\n",
    "\n",
    "    return evolution, torch.Tensor(lengths), torch.Tensor(labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dvib(nn.Module):\n",
    "    def __init__(self,k,out_channels, hidden_size):\n",
    "        super(dvib, self).__init__()\n",
    "        \n",
    "        self.conv = torch.nn.Conv2d(in_channels=1,\n",
    "                            out_channels = out_channels,\n",
    "                            kernel_size = (1,20),\n",
    "                            stride=(1,1),\n",
    "                            padding=(0,0),\n",
    "                            )\n",
    "        \n",
    "        self.rnn = torch.nn.GRU(input_size = out_channels,  \n",
    "                                hidden_size = hidden_size,\n",
    "                                num_layers = 2,\n",
    "                                bidirectional = True,\n",
    "                                batch_first = True,\n",
    "                                dropout = 0.2\n",
    "                              )\n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_size*4, hidden_size*4)\n",
    "        self.enc_mean = nn.Linear(hidden_size*4+578,k)\n",
    "        self.enc_std = nn.Linear(hidden_size*4+578,k)\n",
    "        self.dec = nn.Linear(k, 2)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.constant_(self.fc1.bias, 0.0)\n",
    "        nn.init.xavier_uniform_(self.enc_mean.weight)\n",
    "        nn.init.constant_(self.enc_mean.bias, 0.0)\n",
    "        nn.init.xavier_uniform_(self.enc_std.weight)\n",
    "        nn.init.constant_(self.enc_std.bias, 0.0)\n",
    "        nn.init.xavier_uniform_(self.dec.weight)\n",
    "        nn.init.constant_(self.dec.bias, 0.0)\n",
    "   \n",
    "    def cnn_gru(self,x,lens):\n",
    "#         print(x.shape)\n",
    "        x = x.unsqueeze(1)\n",
    "#         print('after first unsqueeze: ', x.shape)\n",
    "        x = self.conv(x)\n",
    "#         print('after conv: ', x.shape)   \n",
    "        x = torch.nn.ReLU()(x)\n",
    "#         print('shape after relu: ', x.shape,type(x))\n",
    "        x = x.squeeze(3)\n",
    "#         print('shape after squeeze: ', x.shape)\n",
    "#         x = x.view(x.size(0),-1)\n",
    "        x = x.permute(0,2,1)\n",
    "#         print('shape after permute: ', x.shape)\n",
    "#         print(type(lens))\n",
    "        gru_input = pack_padded_sequence(x,lens,batch_first=True)\n",
    "        output, hidden = self.rnn(gru_input)\n",
    "#         print('hidden layer: ', hidden.shape)\n",
    "        output_all = torch.cat([hidden[-1],hidden[-2],hidden[-3],hidden[-4]],dim=1)\n",
    "#         print(\"output_all.shape:\",output_all.shape)    \n",
    "        return output_all\n",
    "        \n",
    "    def forward(self, pssm, lengths, FEGS): \n",
    "        cnn_vectors = self.cnn_gru(pssm, lengths)\n",
    "        feature_vec = torch.cat([cnn_vectors, FEGS], dim = 1)\n",
    "      \n",
    "        enc_mean, enc_std = self.enc_mean(feature_vec), f.softplus(self.enc_std(feature_vec)-5)\n",
    "        eps = torch.randn_like(enc_std)\n",
    "        latent = enc_mean + enc_std*eps\n",
    "        \n",
    "        outputs = f.sigmoid(self.dec(latent))\n",
    "#         print(outputs.shape)\n",
    "\n",
    "        return outputs, enc_mean, enc_std, latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model object\n",
    "device = torch.device('cpu')\n",
    "out_channels = 128\n",
    "hidden_size = 512\n",
    "beta = 2#0，1，3，4，5，6\n",
    "k = 1024\n",
    "model = dvib(k, out_channels, hidden_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBUnpickler(pickle.Unpickler):\n",
    "\n",
    "    def __init__(self, file):\n",
    "        super().__init__(file)\n",
    "\n",
    "    def persistent_load(self, pid):\n",
    "        # This method is invoked whenever a persistent ID is encountered.\n",
    "        # Here, pid is the tuple returned by DBPickler.\n",
    "        lib, className, file, device, unknown = pid\n",
    "        return className()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../OriginalModel/archive/data.pkl', 'rb') as file:\n",
    "    pretrainedModel = DBUnpickler(file).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model weights with weights from ToxIBTL paper\n",
    "for param, weights in zip(model.parameters(), pretrainedModel['model'].values()):\n",
    "    param.data = weights.nan_to_num()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve FEGS embeddings for test set\n",
    "test_fegs = []\n",
    "for i in range(350):\n",
    "    test_data = sio.loadmat('../../data/multiFEGS/test-chunks/' + str(i) + '.mat')['FV']\n",
    "    test_fegs.append(test_data)\n",
    "\n",
    "test_FEGS = torch.Tensor(normalization(np.vstack(test_fegs))).nan_to_num()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: takes a hot second to run, try to run ONCE\n",
    "test_path = './newData/test.csv'\n",
    "test_pssm, test_len, test_label = make_tensor(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = DataLoader(TensorDataset(test_pssm, test_len, test_FEGS, test_label), batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1 of 350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/PyMOL.app/Contents/envs/tox/lib/python3.9/site-packages/torch/nn/functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 2 of 350\n",
      "batch 3 of 350\n",
      "batch 4 of 350\n",
      "batch 5 of 350\n",
      "batch 6 of 350\n",
      "batch 7 of 350\n",
      "batch 8 of 350\n",
      "batch 9 of 350\n",
      "batch 10 of 350\n",
      "batch 11 of 350\n",
      "batch 12 of 350\n",
      "batch 13 of 350\n",
      "batch 14 of 350\n",
      "batch 15 of 350\n",
      "batch 16 of 350\n",
      "batch 17 of 350\n",
      "batch 18 of 350\n",
      "batch 19 of 350\n",
      "batch 20 of 350\n",
      "batch 21 of 350\n",
      "batch 22 of 350\n",
      "batch 23 of 350\n",
      "batch 24 of 350\n",
      "batch 25 of 350\n",
      "batch 26 of 350\n",
      "batch 27 of 350\n",
      "batch 28 of 350\n",
      "batch 29 of 350\n",
      "batch 30 of 350\n",
      "batch 31 of 350\n",
      "batch 32 of 350\n",
      "batch 33 of 350\n",
      "batch 34 of 350\n",
      "batch 35 of 350\n",
      "batch 36 of 350\n",
      "batch 37 of 350\n",
      "batch 38 of 350\n",
      "batch 39 of 350\n",
      "batch 40 of 350\n",
      "batch 41 of 350\n",
      "batch 42 of 350\n",
      "batch 43 of 350\n",
      "batch 44 of 350\n",
      "batch 45 of 350\n",
      "batch 46 of 350\n",
      "batch 47 of 350\n",
      "batch 48 of 350\n",
      "batch 49 of 350\n",
      "batch 50 of 350\n",
      "batch 51 of 350\n",
      "batch 52 of 350\n",
      "batch 53 of 350\n",
      "batch 54 of 350\n",
      "batch 55 of 350\n",
      "batch 56 of 350\n",
      "batch 57 of 350\n",
      "batch 58 of 350\n",
      "batch 59 of 350\n",
      "batch 60 of 350\n",
      "batch 61 of 350\n",
      "batch 62 of 350\n",
      "batch 63 of 350\n",
      "batch 64 of 350\n",
      "batch 65 of 350\n",
      "batch 66 of 350\n",
      "batch 67 of 350\n",
      "batch 68 of 350\n",
      "batch 69 of 350\n",
      "batch 70 of 350\n",
      "batch 71 of 350\n",
      "batch 72 of 350\n",
      "batch 73 of 350\n",
      "batch 74 of 350\n",
      "batch 75 of 350\n",
      "batch 76 of 350\n",
      "batch 77 of 350\n",
      "batch 78 of 350\n",
      "batch 79 of 350\n",
      "batch 80 of 350\n",
      "batch 81 of 350\n",
      "batch 82 of 350\n",
      "batch 83 of 350\n",
      "batch 84 of 350\n",
      "batch 85 of 350\n",
      "batch 86 of 350\n",
      "batch 87 of 350\n",
      "batch 88 of 350\n",
      "batch 89 of 350\n",
      "batch 90 of 350\n",
      "batch 91 of 350\n",
      "batch 92 of 350\n",
      "batch 93 of 350\n",
      "batch 94 of 350\n",
      "batch 95 of 350\n",
      "batch 96 of 350\n",
      "batch 97 of 350\n",
      "batch 98 of 350\n",
      "batch 99 of 350\n",
      "batch 100 of 350\n",
      "batch 101 of 350\n",
      "batch 102 of 350\n",
      "batch 103 of 350\n",
      "batch 104 of 350\n",
      "batch 105 of 350\n",
      "batch 106 of 350\n",
      "batch 107 of 350\n",
      "batch 108 of 350\n",
      "batch 109 of 350\n",
      "batch 110 of 350\n",
      "batch 111 of 350\n",
      "batch 112 of 350\n",
      "batch 113 of 350\n",
      "batch 114 of 350\n",
      "batch 115 of 350\n",
      "batch 116 of 350\n",
      "batch 117 of 350\n",
      "batch 118 of 350\n",
      "batch 119 of 350\n",
      "batch 120 of 350\n",
      "batch 121 of 350\n",
      "batch 122 of 350\n",
      "batch 123 of 350\n",
      "batch 124 of 350\n",
      "batch 125 of 350\n",
      "batch 126 of 350\n",
      "batch 127 of 350\n",
      "batch 128 of 350\n",
      "batch 129 of 350\n",
      "batch 130 of 350\n",
      "batch 131 of 350\n",
      "batch 132 of 350\n",
      "batch 133 of 350\n",
      "batch 134 of 350\n",
      "batch 135 of 350\n",
      "batch 136 of 350\n",
      "batch 137 of 350\n",
      "batch 138 of 350\n",
      "batch 139 of 350\n",
      "batch 140 of 350\n",
      "batch 141 of 350\n",
      "batch 142 of 350\n",
      "batch 143 of 350\n",
      "batch 144 of 350\n",
      "batch 145 of 350\n",
      "batch 146 of 350\n",
      "batch 147 of 350\n",
      "batch 148 of 350\n",
      "batch 149 of 350\n",
      "batch 150 of 350\n",
      "batch 151 of 350\n",
      "batch 152 of 350\n",
      "batch 153 of 350\n",
      "batch 154 of 350\n",
      "batch 155 of 350\n",
      "batch 156 of 350\n",
      "batch 157 of 350\n",
      "batch 158 of 350\n",
      "batch 159 of 350\n",
      "batch 160 of 350\n",
      "batch 161 of 350\n",
      "batch 162 of 350\n",
      "batch 163 of 350\n",
      "batch 164 of 350\n",
      "batch 165 of 350\n",
      "batch 166 of 350\n",
      "batch 167 of 350\n",
      "batch 168 of 350\n",
      "batch 169 of 350\n",
      "batch 170 of 350\n",
      "batch 171 of 350\n",
      "batch 172 of 350\n",
      "batch 173 of 350\n",
      "batch 174 of 350\n",
      "batch 175 of 350\n",
      "batch 176 of 350\n",
      "batch 177 of 350\n",
      "batch 178 of 350\n",
      "batch 179 of 350\n",
      "batch 180 of 350\n",
      "batch 181 of 350\n",
      "batch 182 of 350\n",
      "batch 183 of 350\n",
      "batch 184 of 350\n",
      "batch 185 of 350\n",
      "batch 186 of 350\n",
      "batch 187 of 350\n",
      "batch 188 of 350\n",
      "batch 189 of 350\n",
      "batch 190 of 350\n",
      "batch 191 of 350\n",
      "batch 192 of 350\n",
      "batch 193 of 350\n",
      "batch 194 of 350\n",
      "batch 195 of 350\n",
      "batch 196 of 350\n",
      "batch 197 of 350\n",
      "batch 198 of 350\n",
      "batch 199 of 350\n",
      "batch 200 of 350\n",
      "batch 201 of 350\n",
      "batch 202 of 350\n",
      "batch 203 of 350\n",
      "batch 204 of 350\n",
      "batch 205 of 350\n",
      "batch 206 of 350\n",
      "batch 207 of 350\n",
      "batch 208 of 350\n",
      "batch 209 of 350\n",
      "batch 210 of 350\n",
      "batch 211 of 350\n",
      "batch 212 of 350\n",
      "batch 213 of 350\n",
      "batch 214 of 350\n",
      "batch 215 of 350\n",
      "batch 216 of 350\n",
      "batch 217 of 350\n",
      "batch 218 of 350\n",
      "batch 219 of 350\n",
      "batch 220 of 350\n",
      "batch 221 of 350\n",
      "batch 222 of 350\n",
      "batch 223 of 350\n",
      "batch 224 of 350\n",
      "batch 225 of 350\n",
      "batch 226 of 350\n",
      "batch 227 of 350\n",
      "batch 228 of 350\n",
      "batch 229 of 350\n",
      "batch 230 of 350\n",
      "batch 231 of 350\n",
      "batch 232 of 350\n",
      "batch 233 of 350\n",
      "batch 234 of 350\n",
      "batch 235 of 350\n",
      "batch 236 of 350\n",
      "batch 237 of 350\n",
      "batch 238 of 350\n",
      "batch 239 of 350\n",
      "batch 240 of 350\n",
      "batch 241 of 350\n",
      "batch 242 of 350\n",
      "batch 243 of 350\n",
      "batch 244 of 350\n",
      "batch 245 of 350\n",
      "batch 246 of 350\n",
      "batch 247 of 350\n",
      "batch 248 of 350\n",
      "batch 249 of 350\n",
      "batch 250 of 350\n",
      "batch 251 of 350\n",
      "batch 252 of 350\n",
      "batch 253 of 350\n",
      "batch 254 of 350\n",
      "batch 255 of 350\n",
      "batch 256 of 350\n",
      "batch 257 of 350\n",
      "batch 258 of 350\n",
      "batch 259 of 350\n",
      "batch 260 of 350\n",
      "batch 261 of 350\n",
      "batch 262 of 350\n",
      "batch 263 of 350\n",
      "batch 264 of 350\n",
      "batch 265 of 350\n",
      "batch 266 of 350\n",
      "batch 267 of 350\n",
      "batch 268 of 350\n",
      "batch 269 of 350\n",
      "batch 270 of 350\n",
      "batch 271 of 350\n",
      "batch 272 of 350\n",
      "batch 273 of 350\n",
      "batch 274 of 350\n",
      "batch 275 of 350\n",
      "batch 276 of 350\n",
      "batch 277 of 350\n",
      "batch 278 of 350\n",
      "batch 279 of 350\n",
      "batch 280 of 350\n",
      "batch 281 of 350\n",
      "batch 282 of 350\n",
      "batch 283 of 350\n",
      "batch 284 of 350\n",
      "batch 285 of 350\n",
      "batch 286 of 350\n",
      "batch 287 of 350\n",
      "batch 288 of 350\n",
      "batch 289 of 350\n",
      "batch 290 of 350\n",
      "batch 291 of 350\n",
      "batch 292 of 350\n",
      "batch 293 of 350\n",
      "batch 294 of 350\n",
      "batch 295 of 350\n",
      "batch 296 of 350\n",
      "batch 297 of 350\n",
      "batch 298 of 350\n",
      "batch 299 of 350\n",
      "batch 300 of 350\n",
      "batch 301 of 350\n",
      "batch 302 of 350\n",
      "batch 303 of 350\n",
      "batch 304 of 350\n",
      "batch 305 of 350\n",
      "batch 306 of 350\n",
      "batch 307 of 350\n",
      "batch 308 of 350\n",
      "batch 309 of 350\n",
      "batch 310 of 350\n",
      "batch 311 of 350\n",
      "batch 312 of 350\n",
      "batch 313 of 350\n",
      "batch 314 of 350\n",
      "batch 315 of 350\n",
      "batch 316 of 350\n",
      "batch 317 of 350\n",
      "batch 318 of 350\n",
      "batch 319 of 350\n",
      "batch 320 of 350\n",
      "batch 321 of 350\n",
      "batch 322 of 350\n",
      "batch 323 of 350\n",
      "batch 324 of 350\n",
      "batch 325 of 350\n",
      "batch 326 of 350\n",
      "batch 327 of 350\n",
      "batch 328 of 350\n",
      "batch 329 of 350\n",
      "batch 330 of 350\n",
      "batch 331 of 350\n",
      "batch 332 of 350\n",
      "batch 333 of 350\n",
      "batch 334 of 350\n",
      "batch 335 of 350\n",
      "batch 336 of 350\n",
      "batch 337 of 350\n",
      "batch 338 of 350\n",
      "batch 339 of 350\n",
      "batch 340 of 350\n",
      "batch 341 of 350\n",
      "batch 342 of 350\n",
      "batch 343 of 350\n",
      "batch 344 of 350\n",
      "batch 345 of 350\n",
      "batch 346 of 350\n",
      "batch 347 of 350\n",
      "batch 348 of 350\n",
      "batch 349 of 350\n"
     ]
    }
   ],
   "source": [
    "# evaluate model\n",
    "# note: takes a couple hours to run on a local computer\n",
    "correct = 0\n",
    "y_pre = []\n",
    "y_test = []\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (sequences, lengths, FEGS, labels) in enumerate(test_data):\n",
    "        if (len(lengths) != 100):\n",
    "            break\n",
    "        print('batch ' + str(batch_idx+1) + ' of ' + str(len(test_data)))\n",
    "        seq_lengths, perm_idx = lengths.sort(dim=0, descending=True)\n",
    "        seq_tensor = sequences[perm_idx].to(device)\n",
    "        FEGS_tensor = FEGS[perm_idx].to(device)\n",
    "        label = labels[perm_idx].long().to(device)\n",
    "        y_test.extend(label.cpu().detach().numpy())\n",
    "\n",
    "        y_pred, end_means, enc_stds, latent = model(seq_tensor, seq_lengths, FEGS_tensor)\n",
    "        y_pre.extend(y_pred.argmax(dim=1).cpu().detach().numpy())\n",
    "\n",
    "        _, pred = torch.max(y_pred, 1) \n",
    "\n",
    "        correct += pred.eq(label).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./y_pre.txt', 'w') as y_pre_f:\n",
    "#     for i in y_pre:\n",
    "#         y_pre_f.write(str(i) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./y_test.txt', 'w') as y_test_f:\n",
    "#     for i, label in enumerate(test_label):\n",
    "#         if i < 41000:\n",
    "#             y_test_f.write(str(int(label)) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pre = []\n",
    "# with open('./y_pre.txt') as file:\n",
    "#     for line in file.readlines():\n",
    "#         y_pre.append(int(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test = []\n",
    "# with open('./y_test.txt') as file:\n",
    "#     for line in file.readlines():\n",
    "#         y_test.append(int(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.503323782234957\n",
      "ppv:  0.5026793431287814\n",
      "npv:  0.5039612425192362\n",
      "sensitivity:  0.5006025133413669\n",
      "specificity:  0.5060378870256967\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "tp = 0\n",
    "fp = 0\n",
    "tn = 0\n",
    "fn = 0\n",
    "for pred, label in zip(y_pre, y_test):\n",
    "    if pred == label:\n",
    "        correct += 1\n",
    "    if label == 1 and pred == 1:\n",
    "        tp += 1\n",
    "    elif label == 1 and pred == 0:\n",
    "        fn += 1\n",
    "    elif label == 0 and pred == 0:\n",
    "        tn += 1\n",
    "    elif label == 0 and pred == 1:\n",
    "        fp += 1\n",
    "\n",
    "print('accuracy: ', correct / len(y_test))\n",
    "print('ppv: ', tp / (tp + fp))\n",
    "print('npv: ', tn / (tn + fn))\n",
    "print('sensitivity: ', tp / (tp + fn))\n",
    "print('specificity: ', tn / (tn + fp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tox",
   "language": "python",
   "name": "tox"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
